Big Data Challenges,
Tanggung jawab utama seorang data engineers adalah membuat sebuah pipelines data work-flow yang scalable dan reliable. Tantangan dari ini dikenal sebagai 4Vs, variety, volume, velocity, dan veracity.
	variety : data datang dari berberbagi sumber dan format, yg artinya pipelines harus dapat mengatur data tersebut disimpan secara baik dan tidak
	duplikat.
	volume : disebabkan data yang sangat banyak tersebut, pipelines harus dapat menampungnya sebesar apapun data yang masuk.
	velocity : data harus diproses dengan cepat dalam waktu yang hampir mendekati real-time.
	veracity : kualitas data yang disimpan harus terjamin dalam bentuk yang konsisten dan rapi.
	sehingga tahapan ini mempelajari pembuatan jalur data (pipelines) untuk menampung data secara scalable dan reliable.


Message-oriented architetcure
Pub/Sub : menerima pesan dari berbagai alat stream (pengumpul data), IoT (publisher) - pelanggan yang memakai data atau pesan tadi, analtik tools (subscriber), 

data streaming : data real time (topik) dari sistem (publisher) ke sistem lain (subscriber)

langkah streaming data
data - masuk ke pub/sub (kontak pertama dengan sistem), read, store, dan broadcast - pipeline (transform) - output (BigQuery) - visualisasikan - monitoring

Designing stream pipeline
DataFlow : salah satu produk Google Cloud untuk membuat pipeline baik stream data (real-time data) atau batch data.
	- melakukan proses ETL, extract, transform and load

batch data : metode pemrosesan data dalam blok besar atau "batch" yang dikumpulkan selama periode waktu tertentu.

Apache Beam : open source untuk pembuatan pipeline, termasuk proses ETL, batch, dan stream proses.

Implementing streaming pipeline on Cloud DataFlow
apache beam = membuat pipelines untuk pemprosesan data, membutugkan engine untuk eksekusi proses?
solution : DataFlow from Google cloud, mengeksekusi apache beam pipeline dalam cloud.

DataFlow : No Ops and Serverless, tidak membutuh management team and bisa akses secara cloud

Langkah: 
- apache Beam
- DataFlow : using template is one of solution to create the pipeline process
	benefit and done by datadflow:
	o graph optimazion
	o work scheduler  : memberikan tugas kepada workers
	o auto scaler	
	o auto healing
	o work rebalancing
	o compute & storage	: mengkomputasi dan simpan data dalam bucket
- bigQuery 
- looker and looker studio